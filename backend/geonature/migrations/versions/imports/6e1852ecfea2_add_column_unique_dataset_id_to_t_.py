"""add column unique_dataset_id to t_imports_synthese and insert into bib_fields and cor_entity_field

Revision ID: 6e1852ecfea2
Revises: 8b149244d586
Create Date: 2024-03-04 12:31:00.861460

"""

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.schema import Table, MetaData

# revision identifiers, used by Alembic.
revision = "6e1852ecfea2"
down_revision = "8b149244d586"
branch_labels = None
depends_on = None


def upgrade():
    meta = MetaData(bind=op.get_bind())

    # Add columns to t_imports_synthese table
    with op.batch_alter_table("t_imports_synthese", schema="gn_imports") as batch_op:
        batch_op.add_column(sa.Column("src_unique_dataset_id", sa.String))
        batch_op.add_column(sa.Column("unique_dataset_id", UUID(as_uuid=True)))
        batch_op.add_column(sa.Column("id_dataset", sa.Integer))
    # Fetch id_destination for 'synthese' from bib_destinations table
    destination = Table("bib_destinations", meta, autoload=True, schema="gn_imports")
    id_dest_synthese = (
        op.get_bind()
        .execute(sa.select([destination.c.id_destination]).where(destination.c.code == "synthese"))
        .scalar()
    )
    # Fetch id_entity_observation for id_destination from bib_entities table
    entity = Table("bib_entities", meta, autoload=True, schema="gn_imports")
    id_entity_observation = (
        op.get_bind()
        .execute(sa.select([entity.c.id_entity]).where(entity.c.id_destination == id_dest_synthese))
        .scalar()
    )

    # Fetch id_theme_general from bib_themes table
    theme = Table("bib_themes", meta, autoload=True, schema="gn_imports")
    id_theme_general = (
        op.get_bind()
        .execute(sa.select([theme.c.id_theme]).where(theme.c.name_theme == "general_info"))
        .scalar()
    )

    # Fetch id_field for 'unique_dataset_id' from bib_fields table
    field = Table("bib_fields", meta, autoload=True, schema="gn_imports")
    list_field_to_insert = [
        (
            {
                "name_field": "unique_dataset_id",
                "fr_label": "Identifiant JDD (UUID)",
                "mandatory": False,
                "autogenerated": False,
                "display": False,
                "mnemonique": None,
                "source_field": "src_unique_dataset_id",
                "dest_field": "unique_dataset_id",
            },
            {
                id_entity_observation: {
                    "id_theme": id_theme_general,
                    "order_field": 3,
                    "comment": "Correspondance champs standard: metadonneeId ou jddMetaId",
                },
            },
        ),
        (
            {
                "name_field": "id_dataset",
                "fr_label": "Identifiant JDD",
                "mandatory": False,
                "autogenerated": False,
                "display": False,
                "mnemonique": None,
                "source_field": None,
                "dest_field": "id_dataset",
            },
            {
                id_entity_observation: {
                    "id_theme": id_theme_general,
                    "order_field": 3,
                    "comment": "",
                },
            },
        ),
    ]
    # insert_data = {"id_destination": id_dest_synthese, **field_unique_dataset_id_info}

    id_fields = [
        id_field
        for id_field, in op.get_bind()
        .execute(
            sa.insert(field)
            .values(
                [{"id_destination": id_dest_synthese, **field} for field, _ in list_field_to_insert]
            )
            .returning(field.c.id_field)
        )
        .fetchall()
    ]

    # Insert data into cor_entity_field table
    cor_entity_field = Table("cor_entity_field", meta, autoload=True, schema="gn_imports")
    cor_entity_field = Table("cor_entity_field", meta, autoload=True, schema="gn_imports")
    op.execute(
        sa.insert(cor_entity_field).values(
            [
                {"id_entity": id_entity, "id_field": id_field, **props}
                for id_field, field_entities in zip(id_fields, list_field_to_insert)
                for id_entity, props in field_entities[1].items()
            ]
        )
    )

    # Update model contentmapping to add unique_dataset_id
    t_mappings = Table("t_mappings", meta, autoload=True, schema="gn_imports")

    id_t_mapping_synthese = (
        op.get_bind()
        .execute(sa.select([t_mappings.c.id]).where(t_mappings.c.label == "Synthese GeoNature"))
        .scalar()
    )

    update_query = sa.text(
        """
        UPDATE gn_imports.t_fieldmappings
        SET values = values::jsonb || '{"unique_dataset_id": "unique_dataset_id"}'::jsonb
        WHERE id = :id_t_mapping_synthese
        """
    )

    op.get_bind().execute(update_query, id_t_mapping_synthese=id_t_mapping_synthese)


def downgrade():
    meta = MetaData(bind=op.get_bind())

    # Drop columns from t_imports_synthese table
    with op.batch_alter_table("t_imports_synthese", schema="gn_imports") as batch_op:
        batch_op.drop_column("unique_dataset_id")
        batch_op.drop_column("src_unique_dataset_id")
        batch_op.drop_column("id_dataset")

    # Fetch id_destination for 'synthese' from bib_destinations table
    destination = Table("bib_destinations", meta, autoload=True, schema="gn_imports")
    id_dest_synthese = (
        op.get_bind()
        .execute(sa.select([destination.c.id_destination]).where(destination.c.code == "synthese"))
        .scalar()
    )

    # Fetch id_entity_observation for id_destination from bib_entities table
    entity = Table("bib_entities", meta, autoload=True, schema="gn_imports")
    id_entity_observation = (
        op.get_bind()
        .execute(sa.select([entity.c.id_entity]).where(entity.c.id_destination == id_dest_synthese))
        .scalar()
    )

    # Fetch id_fields inserted into bib_fields table
    field = Table("bib_fields", meta, autoload=True, schema="gn_imports")
    id_fields = (
        op.get_bind()
        .execute(
            sa.select([field.c.id_field]).where(
                sa.or_(
                    sa.and_(
                        field.c.name_field == "unique_dataset_id",
                        field.c.id_destination == id_dest_synthese,
                    ),
                    sa.and_(
                        field.c.name_field == "id_dataset",
                        field.c.id_destination == id_dest_synthese,
                    ),
                )
            )
        )
        .scalars()
        .all()
    )

    # Delete rows from cor_entity_field based on matching list of id_fields
    cor_entity_field = Table("cor_entity_field", meta, autoload=True, schema="gn_imports")
    op.execute(
        cor_entity_field.delete().where(
            sa.and_(
                cor_entity_field.c.id_entity == id_entity_observation,
                cor_entity_field.c.id_field.in_(id_fields),
            )
        )
    )

    op.execute(field.delete().where(field.c.id_field.in_(id_fields)))

    t_mappings = Table("t_mappings", meta, autoload=True, schema="gn_imports")

    # Get the ID of the "Synthese GeoNature" mapping
    id_t_mapping_synthese = (
        op.get_bind()
        .execute(sa.select([t_mappings.c.id]).where(t_mappings.c.label == "Synthese GeoNature"))
        .scalar()
    )

    revert_query = sa.text(
        """
        UPDATE gn_imports.t_fieldmappings
        SET values = values::jsonb - 'unique_dataset_id'
        WHERE id = :id_t_mapping_synthese
        """
    )

    op.get_bind().execute(revert_query, id_t_mapping_synthese=id_t_mapping_synthese)
